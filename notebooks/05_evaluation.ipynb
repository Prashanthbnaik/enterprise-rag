{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47b69df",
   "metadata": {},
   "source": [
    "# Evaluation Framework\n",
    "\n",
    "This notebook evaluates the production RAG system using:\n",
    "\n",
    "- Custom Evaluation\n",
    "- Latency tracking\n",
    "- Precision analysis\n",
    "- Faithfulness validation\n",
    "\n",
    "The pipeline under evaluation is imported from `app.pipeline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aaacdb",
   "metadata": {},
   "source": [
    "## Import Production Pipeline\n",
    "\n",
    "The `rag_pipeline` is imported from the application module to validate the production setup.\n",
    "\n",
    "This confirms that retrieval, reranking, guardrails, and generation are fully modularized and executable outside the notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da065e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741d1007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0223 09:29:17.643000 17760 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from app.pipeline import rag_pipeline\n",
    "from app.pipeline import documents\n",
    "from app.generation import llm_client\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4be54",
   "metadata": {},
   "source": [
    "## Define Evaluation Query Set\n",
    "\n",
    "A diverse evaluation set is defined to test retrieval and generation performance across different question types.\n",
    "\n",
    "The queries are designed to cover:\n",
    "\n",
    "- Specific limitation-focused queries  \n",
    "- Mechanism explanations  \n",
    "- Comparative reasoning  \n",
    "- General conceptual questions  \n",
    "- Higher retrieval difficulty scenarios  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a7c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_queries = [\n",
    "    \"How many AI publications were reported in 2023?\",\n",
    "    \"What percentage of AI publications in 2023 came from academia?\",\n",
    "    \"How has the total number of AI publications changed between 2013 and 2023?\",\n",
    "    \"How has the training dataset size of notable AI models evolved over time?\",\n",
    "    \"How much power was required to train frontier AI models in recent years?\",\n",
    "    \"What trends are observed in global AI patent growth?\",\n",
    "    \"How has China’s share of AI publications and citations changed over time?\",\n",
    "    \"What does the report say about public opinion regarding AI’s impact on jobs?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ceab1",
   "metadata": {},
   "source": [
    "This structured evaluation ensures the pipeline is tested beyond simple keyword matching and demonstrates robustness across varied information demands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60606814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.generation.llm_client import GroqLLMClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm_client = GroqLLMClient(\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6d001",
   "metadata": {},
   "source": [
    "## Evaluation Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0967eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfulness_prompt(contexts, answer):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator.\n",
    "\n",
    "Context:\n",
    "{contexts}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Is the answer fully supported by the context?\n",
    "Respond with ONLY one word: YES or NO.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a41286",
   "metadata": {},
   "source": [
    "## Context Precision Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75482dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_precision_prompt(question, contexts):\n",
    "    return f\"\"\"\n",
    "You are evaluating retrieval quality.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Retrieved Context:\n",
    "{contexts}\n",
    "\n",
    "Is the retrieved context relevant to the question?\n",
    "Respond with ONLY one word: YES or NO.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95aeb13",
   "metadata": {},
   "source": [
    "## Answer Relevancy Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6237811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_relevancy_prompt(question, answer):\n",
    "    return f\"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Does the answer directly address the question?\n",
    "Respond with ONLY one word: YES or NO.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218197a9",
   "metadata": {},
   "source": [
    "## Judge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765be31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(prompt):\n",
    "    response = llm_client.generate(\n",
    "        prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=5\n",
    "    )\n",
    "    \n",
    "    output = response[\"text\"].strip().upper()\n",
    "    \n",
    "    if \"YES\" in output:\n",
    "        return \"YES\"\n",
    "    elif \"NO\" in output:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"INVALID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398dac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>hallucination_flag</th>\n",
       "      <th>latency_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many AI publications were reported in 2023?</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>3.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What percentage of AI publications in 2023 cam...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>4.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How has the total number of AI publications ch...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How has the training dataset size of notable A...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How much power was required to train frontier ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What trends are observed in global AI patent g...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has China’s share of AI publications and c...</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>5.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What does the report say about public opinion ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>YES</td>\n",
       "      <td>0</td>\n",
       "      <td>4.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question faithfulness  \\\n",
       "0    How many AI publications were reported in 2023?          YES   \n",
       "1  What percentage of AI publications in 2023 cam...          YES   \n",
       "2  How has the total number of AI publications ch...          YES   \n",
       "3  How has the training dataset size of notable A...          YES   \n",
       "4  How much power was required to train frontier ...          YES   \n",
       "5  What trends are observed in global AI patent g...          YES   \n",
       "6  How has China’s share of AI publications and c...           NO   \n",
       "7  What does the report say about public opinion ...          YES   \n",
       "\n",
       "  context_precision answer_relevancy  hallucination_flag  latency_sec  \n",
       "0               YES              YES                   0        3.702  \n",
       "1               YES              YES                   0        4.580  \n",
       "2               YES              YES                   0        0.838  \n",
       "3               YES              YES                   0        0.720  \n",
       "4               YES              YES                   0        0.736  \n",
       "5               YES              YES                   0        0.749  \n",
       "6                NO               NO                   0        5.022  \n",
       "7               YES              YES                   0        4.077  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for query in evaluation_queries:\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = rag_pipeline(query)\n",
    "    latency = round(time.time() - start_time, 3)\n",
    "\n",
    "    answer = response.get(\"answer\", \"\")\n",
    "    contexts = response.get(\"contexts\", [])\n",
    "    combined_context = \" \".join(contexts)\n",
    "\n",
    "    #  Metrics\n",
    "    faithfulness_score = judge(\n",
    "        faithfulness_prompt(combined_context, answer)\n",
    "    )\n",
    "\n",
    "    context_precision_score = judge(\n",
    "        context_precision_prompt(query, combined_context)\n",
    "    )\n",
    "\n",
    "    answer_relevancy_score = judge(\n",
    "        answer_relevancy_prompt(query, answer)\n",
    "    )\n",
    "\n",
    "    if answer.startswith(\"Insufficient\"):\n",
    "        hallucination_flag = 0\n",
    "    elif faithfulness_score == \"NO\":\n",
    "        hallucination_flag = 1\n",
    "    else:\n",
    "        hallucination_flag = 0\n",
    "\n",
    "    evaluation_results.append({\n",
    "        \"question\": query,\n",
    "        \"faithfulness\": faithfulness_score,\n",
    "        \"context_precision\": context_precision_score,\n",
    "        \"answer_relevancy\": answer_relevancy_score,\n",
    "        \"hallucination_flag\": hallucination_flag,\n",
    "        \"latency_sec\": latency\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(evaluation_results)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4287830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Faithfulness (%)': 87.5,\n",
       " 'Context Precision (%)': 87.5,\n",
       " 'Answer Relevancy (%)': 87.5,\n",
       " 'Hallucination Rate (%)': 0.0,\n",
       " 'Average Latency (sec)': 2.553}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    \"Faithfulness (%)\":\n",
    "        (df_eval[\"faithfulness\"] == \"YES\").mean() * 100,\n",
    "\n",
    "    \"Context Precision (%)\":\n",
    "        (df_eval[\"context_precision\"] == \"YES\").mean() * 100,\n",
    "\n",
    "    \"Answer Relevancy (%)\":\n",
    "        (df_eval[\"answer_relevancy\"] == \"YES\").mean() * 100,\n",
    "\n",
    "    \"Hallucination Rate (%)\":\n",
    "        df_eval[\"hallucination_flag\"].mean() * 100,\n",
    "\n",
    "    \"Average Latency (sec)\":\n",
    "        df_eval[\"latency_sec\"].mean()\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fa826",
   "metadata": {},
   "source": [
    "## Final Evaluation Report\n",
    "\n",
    "System Behavior:\n",
    "- The RAG pipeline demonstrates high grounding reliability.\n",
    "- No hallucinations observed across evaluation queries.\n",
    "- Retrieval precision remains strong for LoRA-related queries.\n",
    "- Retrieval rejection correctly handles unsupported topics.\n",
    "- Average latency remains at 2.5 second.\n",
    "\n",
    "Conclusion:\n",
    "The hybrid retrieval + reranking + guarded generation pipeline \n",
    "achieves production-ready reliability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0165d8",
   "metadata": {},
   "source": [
    "-------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
