{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51afa7b",
   "metadata": {},
   "source": [
    "# Embedding & Vector Store Engineering\n",
    "\n",
    "This notebook focuses on engineering the embedding and vector storage layer of the Hybrid RAG system.\n",
    "\n",
    "We are not simply generating embeddings.  \n",
    "We are benchmarking models, measuring system impact, and making production-grade architectural decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Scope\n",
    "\n",
    "- Compare embedding models  \n",
    "- Measure embedding latency  \n",
    "- Analyze vector dimensionality  \n",
    "- Estimate memory footprint  \n",
    "- Design vector store abstraction  \n",
    "- Build and persist a FAISS index  \n",
    "- Prepare the system for hybrid retrieval  \n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load processed chunks from Previous Notebook  \n",
    "- Evaluate two embedding models under measurable criteria:\n",
    "  - Embedding speed  \n",
    "  - Vector dimensions  \n",
    "  - Memory consumption  \n",
    "- Build a FAISS index for efficient similarity search  \n",
    "- Save the index for reuse  \n",
    "- Design an abstraction layer to allow seamless migration to Pinecone or other managed vector databases  \n",
    "\n",
    "This notebook establishes the semantic retrieval backbone required for scalable and optimized search in the Hybrid RAG architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1549b",
   "metadata": {},
   "source": [
    "## 1 – Load Processed Chunks\n",
    "\n",
    "The processed chunk dataset generated in Notebook 1 is loaded from disk using serialization. This avoids recomputing ingestion and chunking, ensuring faster iteration during embedding experiments.\n",
    "\n",
    "A sanity check is performed to:\n",
    "\n",
    "- Verify the total number of loaded chunks  \n",
    "- Inspect a sample of the chunk content  \n",
    "- Validate metadata integrity  \n",
    "\n",
    "This confirms that the dataset is clean, structured, and ready for embedding benchmarking and vector indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27faf749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1333 chunks.\n",
      "Artificial Intelligence\n",
      "Index Report 2025\n",
      "{'source': 'C:\\\\my_projects\\\\enterprise-rag\\\\data\\\\Stanford AI Index Report.pdf', 'page': 0, 'doc_id': 'bb44519d-f091-4d79-9f95-7e4bed72a9e6'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"processed_chunks.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(documents)} chunks.\")\n",
    "\n",
    "# Sanity check\n",
    "print(documents[0].page_content[:300])\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469a17b",
   "metadata": {},
   "source": [
    "## 2 – Embedding Model Selection\n",
    "\n",
    "Two embedding models are selected for benchmarking to evaluate performance versus semantic quality trade-offs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305648b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prashanth\\miniconda3\\envs\\python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0221 18:05:18.921000 2372 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_a dimension vectors: 384\n",
      "model_b dimension vectors 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # supress all warnings\n",
    "\n",
    "model_a = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"model_a dimension vectors:\", model_a.get_sentence_embedding_dimension())\n",
    "\n",
    "model_b = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "print(\"model_b dimension vectors\", model_b.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd3d03",
   "metadata": {},
   "source": [
    "### Model A – all-MiniLM-L6-v2  \n",
    "- 384-dimensional vectors  \n",
    "- Fast inference  \n",
    "- Low memory footprint  \n",
    "- Widely used production baseline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2ac91",
   "metadata": {},
   "source": [
    "### Model B – all-mpnet-base-v2  \n",
    "- 768-dimensional vectors  \n",
    "- Stronger semantic representation  \n",
    "- Slower inference  \n",
    "- Higher memory consumption  \n",
    "\n",
    "This comparison enables a measurable decision between efficiency and retrieval quality before committing to a production embedding standard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926ca59",
   "metadata": {},
   "source": [
    "## 3 – Embedding Benchmark Framework\n",
    "\n",
    "A modular benchmarking function is implemented to evaluate embedding models under measurable performance criteria.\n",
    "\n",
    "For each model, the function:\n",
    "\n",
    "- Loads the embedding model dynamically  \n",
    "- Extracts chunk text for encoding  \n",
    "- Measures total embedding latency  \n",
    "- Reports embedding matrix shape  \n",
    "- Extracts vector dimensionality  \n",
    "\n",
    "This allows direct comparison of:\n",
    "\n",
    "- Inference speed  \n",
    "- Vector dimensions  \n",
    "- Computational cost  \n",
    "\n",
    "The design is intentionally model-agnostic, enabling easy extension to additional embedding models in future experiments. This benchmarking layer ensures embedding selection is driven by empirical performance rather than assumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4584a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def benchmark_embedding(model_name, documents):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "\n",
    "    print(\"\\nModel:\", model_name)\n",
    "    print(\"Embedding shape:\", embeddings.shape)\n",
    "    print(\"Time taken (s):\", round(latency, 2))\n",
    "    print(\"Embedding dimension:\", embeddings.shape[1])\n",
    "\n",
    "    return embeddings, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8072f",
   "metadata": {},
   "source": [
    "## 4 – Execute Embedding Benchmark\n",
    "\n",
    "The benchmarking function is executed for both selected models to collect empirical performance metrics.\n",
    "\n",
    "This step generates:\n",
    "\n",
    "- Embedding matrices for each model  \n",
    "- Total inference latency  \n",
    "- Vector dimensionality comparison  \n",
    "\n",
    "The results provide a direct trade-off analysis between speed (MiniLM) and semantic strength (MPNet), forming the basis for selecting the production embedding model before vector indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9409d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 42/42 [00:33<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: all-MiniLM-L6-v2\n",
      "Embedding shape: (1333, 384)\n",
      "Time taken (s): 33.39\n",
      "Embedding dimension: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run benchmark on \"all-MiniLM-L6-v2\"\n",
    "embedding_mini, latency_mini = benchmark_embedding(\n",
    "    \"all-MiniLM-L6-v2\",  \n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c525fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 42/42 [04:20<00:00,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: all-mpnet-base-v2\n",
      "Embedding shape: (1333, 768)\n",
      "Time taken (s): 260.58\n",
      "Embedding dimension: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run benckmark on \"all-mpnet-base-v2\"\n",
    "embedding_mpnet, latency_mpnet = benchmark_embedding(\n",
    "    \"all-mpnet-base-v2\",\n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7385812",
   "metadata": {},
   "source": [
    "## 5 – Memory Footprint Estimation\n",
    "\n",
    "To evaluate scalability, the memory footprint of each embedding matrix is calculated in megabytes.\n",
    "\n",
    "This measurement is critical when projecting system behavior at scale. As the number of chunks grows into the millions, embedding dimensionality directly impacts:\n",
    "\n",
    "- RAM consumption  \n",
    "- Vector index size  \n",
    "- Infrastructure cost  \n",
    "- Retrieval latency  \n",
    "\n",
    "Comparing memory usage between MiniLM and MPNet provides a realistic view of production deployment constraints, especially in high-volume enterprise environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6abe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM Memory (MB): 1.95\n",
      "MPNet Memory (MB): 3.91\n"
     ]
    }
   ],
   "source": [
    "def estimate_memory(embeddings):\n",
    "    return embeddings.nbytes / (1024 ** 2)\n",
    "\n",
    "print(\"MiniLM Memory (MB):\", round(estimate_memory(embedding_mini), 2))\n",
    "print(\"MPNet Memory (MB):\", round(estimate_memory(embedding_mpnet), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bcc71",
   "metadata": {},
   "source": [
    "## 6 – Build FAISS Index\n",
    "\n",
    "The FAISS index is initialized using MiniLM embeddings, selected as the likely production candidate based on efficiency and memory considerations.\n",
    "\n",
    "An `IndexFlatL2` index is created with the appropriate vector dimensionality and populated with the embedding matrix.\n",
    "\n",
    "This establishes:\n",
    "\n",
    "- In-memory similarity search capability  \n",
    "- Deterministic L2 distance-based retrieval  \n",
    "- A scalable foundation for semantic search  \n",
    "\n",
    "The total vector count is verified to ensure all embeddings are successfully indexed before implementing the search interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba119f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 42/42 [00:32<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vectors in index: 1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Assume documents is a list of LangChain Document objects\n",
    "# documents[i].page_content contains text\n",
    "\n",
    "texts = [doc.page_content for doc in documents]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Convert to float32 (required by FAISS)\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Normalize embeddings \n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Create FAISS index \n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Total vectors in index:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105c659",
   "metadata": {},
   "source": [
    "## 7 – Semantic Search Interface\n",
    "\n",
    "A reusable search function is implemented to query the FAISS index using semantic similarity.\n",
    "\n",
    "The function:\n",
    "\n",
    "- Encodes the input query using the selected embedding model  \n",
    "- Performs nearest-neighbor search on the FAISS index  \n",
    "- Retrieves the top-k most similar document chunks  \n",
    "- Returns the corresponding `Document` objects  \n",
    "\n",
    "This abstraction separates retrieval logic from the indexing layer, enabling future extensions such as hybrid search, reranking, or database-backed vector stores without modifying the core search interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c909319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, model, index, documents, top_k=5):\n",
    "    if not query.strip():\n",
    "        raise ValueError(\"Query cannot be empty.\")\n",
    "\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "\n",
    "    # Normalize query embedding\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    # Perform search\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for rank, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"score\": float(distance),  # Lower = better (L2 distance)\n",
    "            \"content\": documents[idx].page_content,\n",
    "            \"metadata\": documents[idx].metadata\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2f5e2",
   "metadata": {},
   "source": [
    "## 8 – Validate Retrieval Pipeline\n",
    "\n",
    "The semantic search function is tested using a domain-specific query to evaluate retrieval relevance.\n",
    "\n",
    "This validation step confirms:\n",
    "\n",
    "- Correct query embedding generation  \n",
    "- Proper FAISS nearest-neighbor search execution  \n",
    "- Accurate mapping from index results to original document chunks  \n",
    "\n",
    "Inspecting the top retrieved passages helps assess whether the embedding model captures conceptual similarity effectively.  \n",
    "\n",
    "This qualitative check complements earlier quantitative benchmarks and verifies that the retrieval layer is functioning as expected before introducing hybrid search or reranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62c6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank 1 | L2 Distance: 0.4747\n",
      "--------------------------------------------------------------------------------\n",
      "Table of Contents 36\n",
      "Artificial Intelligence\n",
      "Index Report 2025Chapter 1 Preview\n",
      "By Sector\n",
      "Academic institutions remain the primary source of AI \n",
      "publications worldwide (Figure 1.1.8). In 2013, they accounted \n",
      "for 85.9% of all AI publications, a figure that remained high, \n",
      "at 84.9%, in 2023. Industry contributed 7.1% of AI publications \n",
      "in 2023, followed by government institutions at 4.9% and \n",
      "nonprofit organizations at 1.7%.\n",
      "AI publications in CS (% of total)\n",
      "1.35%, Other\n",
      "1.70%, Nonprot\n",
      "4.90%, Government\n",
      "7.14%, Industry\n",
      "84.91%, Academia\n",
      "AI publications in CS (% of total) by sector, 2013–23\n",
      "Source: AI Index, 2025 | Chart: 2025 AI Index report\n",
      "Figure 1.1.87\n",
      "1.1 Publications\n",
      "Chapter 1: Research and Development\n",
      "================================================================================\n",
      "\n",
      "Rank 2 | L2 Distance: 0.5127\n",
      "--------------------------------------------------------------------------------\n",
      "database. As a result, the numbers in this year’s report differ \n",
      "slightly from those in previous editions.\n",
      "1 Given that there is a \n",
      "significant lag in the collection of publication metadata, and \n",
      "that in some cases it takes until the middle of any given year \n",
      "to fully capture the previous year’s publications, in this year’s \n",
      "report, the AI Index team elected to examine publication \n",
      "trends only through 2023.\n",
      "Overview\n",
      "The following section reports on trends in the total number of \n",
      "English-language AI publications. \n",
      "Total Number of AI Publications\n",
      "Figure 1.1.1 displays the global count of AI publications. These \n",
      "are the publications with a computer science (CS) label in the \n",
      "OpenAlex catalog that were classified by the AI Index as being \n",
      "related to AI.\n",
      "================================================================================\n",
      "\n",
      "Rank 3 | L2 Distance: 0.5155\n",
      "--------------------------------------------------------------------------------\n",
      "Table of Contents 39\n",
      "Artificial Intelligence\n",
      "Index Report 2025Chapter 1 Preview\n",
      "Singapore\n",
      "Israel\n",
      "United Arab Emirates\n",
      "United Kingdom\n",
      "South Korea\n",
      "Canada\n",
      "Hong Kong\n",
      "Germany\n",
      "China\n",
      "United States\n",
      "Number of highly cited publications in top 100\n",
      "Number of highly cited publications in top 100 by select geographic areas, 2021–23\n",
      "Source: AI Index, 2025 | Chart: 2025 AI Index report\n",
      "Figure 1.1.11\n",
      "Top 100 Publications\n",
      "While tracking total AI publications provides a broad view of \n",
      "research activity, focusing on the most-cited papers offers a \n",
      "perspective of the field’s most influential work. This analysis \n",
      "sheds light on where some of the most groundbreaking and \n",
      "influential AI research is emerging. This year, the AI Index \n",
      "identified the 100 most-cited AI publications in 2021, 2022,\n",
      "================================================================================\n",
      "\n",
      "Rank 4 | L2 Distance: 0.5476\n",
      "--------------------------------------------------------------------------------\n",
      "Table of Contents 29\n",
      "Artificial Intelligence\n",
      "Index Report 2025Chapter 1 Preview\n",
      "Number of AI publications in CS (in thousands)\n",
      "Number of AI publications in CS worldwide, 2013–23\n",
      "Source: AI Index, 2025 | Chart: 2025 AI Index report\n",
      "1.1 Publications\n",
      "The figures below show the global count of English-language \n",
      "AI publications from 2010 to 2023, categorized by affiliation \n",
      "type, publication type, and region. New to this year’s report, \n",
      "the AI Index includes a section analyzing trends among the \n",
      "100 most-cited AI publications, which can offer insights \n",
      "into particularly high-impact research. This year, the AI \n",
      "Index analyzed AI publication trends using the OpenAlex \n",
      "database. As a result, the numbers in this year’s report differ \n",
      "slightly from those in previous editions.\n",
      "================================================================================\n",
      "\n",
      "Rank 5 | L2 Distance: 0.5534\n",
      "--------------------------------------------------------------------------------\n",
      "Table of Contents 35\n",
      "Artificial Intelligence\n",
      "Index Report 2025Chapter 1 Preview\n",
      "In 2023, Chinese AI publications accounted for 22.6% of all AI citations, followed by Europe at 20.9% and the United States at \n",
      "13.0% (Figure 1.1.7). As with total AI publications, the late 2010s marked a turning point when China surpassed Europe and the \n",
      "U.S. as the leading source of AI publication citations.\n",
      "AI publication citations in CS (% of total)\n",
      "6.10%, India\n",
      "7.54%, Unknown\n",
      "13.03%, United States\n",
      "20.90%, Europe\n",
      "22.60%, China\n",
      "29.83%, Rest of the world\n",
      "AI publication citations in CS (% of total) by select geographic areas, 2013–23\n",
      "Source: AI Index, 2025 | Chart: 2025 AI Index report\n",
      "Figure 1.1.7\n",
      "1.1 Publications\n",
      "Chapter 1: Research and Development\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"How many AI publications were there in 2023?\"\n",
    "\n",
    "results = search(query, model, index, documents, top_k=5)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\nRank {r['rank']} | L2 Distance: {r['score']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"content\"])\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72d0d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n",
      "5 False\n"
     ]
    }
   ],
   "source": [
    "def contains_limitation_terms(text):\n",
    "    keywords = [\"limitation\", \"drawback\", \"constraint\", \"trade\", \"reduce\"]\n",
    "    return any(k.lower() in text.lower() for k in keywords)\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"rank\"], contains_limitation_terms(r[\"content\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd6d63",
   "metadata": {},
   "source": [
    "## 9 – Persist FAISS Index\n",
    "\n",
    "The FAISS index is serialized and saved to disk for reuse in future sessions.\n",
    "\n",
    "Persisting the index eliminates the need to recompute embeddings and rebuild the vector store during deployment or subsequent experiments.  \n",
    "\n",
    "This step prepares the system for production workflows, where the embedding pipeline and index construction are executed offline, while the search interface operates in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6032e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae851cf7",
   "metadata": {},
   "source": [
    "## Embedding & Vector Store Engineering summary\n",
    "\n",
    "This notebook established the semantic retrieval backbone of the Hybrid RAG system.\n",
    "\n",
    "Completed stages include:\n",
    "\n",
    "- Loading processed chunks from the ingestion pipeline  \n",
    "- Benchmarking two embedding models (MiniLM vs MPNet)  \n",
    "- Measuring embedding latency, dimensionality, and memory footprint  \n",
    "- Selecting MiniLM based on performance-efficiency trade-offs  \n",
    "- Building and validating a FAISS vector index  \n",
    "- Designing a modular search interface  \n",
    "- Persisting the FAISS index for production reuse  \n",
    "\n",
    "The system now has a measurable, scalable embedding layer with a working vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29f02d",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
