{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e152f2",
   "metadata": {},
   "source": [
    "# __Production-Grade Hybrid RAG System with Guardrails, Reranking, and Evaluation Framework__\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "This section initializes all required dependencies for building a production-grade Hybrid Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "The setup supports:\n",
    "\n",
    "- Document ingestion from PDFs and web sources  \n",
    "- Recursive semantic chunking  \n",
    "- Dense embedding generation using Sentence Transformers  \n",
    "- Similarity computation for retrieval and reranking  \n",
    "- Structured logging for observability and debugging  \n",
    "\n",
    "The environment is structured to remain modular, enabling future integration of hybrid retrieval strategies, guardrails, reranking layers, and evaluation pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "643f0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Prashanth\\miniconda3\\envs\\python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Document Loading\n",
    "from langchain.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   # Supress all warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b858ac",
   "metadata": {},
   "source": [
    "## 2. Ingestion Layer Design\n",
    "\n",
    "The ingestion layer is designed to be modular and extensible, allowing multiple document sources (PDFs, web pages, APIs) to be integrated into the RAG pipeline with minimal changes.\n",
    "\n",
    "Each loader follows a consistent interface and returns a standardized list of `Document` objects, ensuring compatibility with downstream chunking and embedding components.\n",
    "\n",
    "\n",
    "### PDF Loader\n",
    "\n",
    "This function loads documents from a PDF file using `PyPDFLoader` and returns structured `Document` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c912723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 'load_pdf' function\n",
    "def load_pdf(file_path: str) -> List[Document]:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()   # loads the document\n",
    "    logger.info(f\"Loaded {len(documents)} pages from PDF.\")\n",
    "    return documents   # returns document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f23e87",
   "metadata": {},
   "source": [
    "### CSV Loader\n",
    "\n",
    "The CSV loader converts structured tabular data into retrievable document units.  \n",
    "Each row is transformed into a single text representation and stored as a `Document` with metadata including the source path and row ID.\n",
    "\n",
    "This ensures structured datasets can seamlessly integrate with PDFs and web content within the same Hybrid RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fa698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 'load_csv' function\n",
    "def load_csv(file_path: str) -> List[Document]:\n",
    "    df = pd.read_csv(file_path)\n",
    "    text_data = df.astype(str).apply(\" \".join, axis=1)\n",
    "\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=row,\n",
    "            metadata={\"source\": file_path, \"row_id\": i}\n",
    "        )\n",
    "        for i, row in enumerate(text_data)\n",
    "    ]\n",
    "\n",
    "    logger.info(f\"Loaded {len(documents)} rows from CSV.\")\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5c340",
   "metadata": {},
   "source": [
    "### Web Loader\n",
    "\n",
    "The web loader enables ingestion of online content directly into the RAG pipeline.  \n",
    "It fetches webpage data and converts it into standardized `Document` objects for downstream chunking and embedding.\n",
    "\n",
    "This allows dynamic knowledge sources (e.g., documentation sites, blogs, internal portals) to be integrated alongside PDFs and structured datasets within the same retrieval architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08328ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 'load_web' function\n",
    "def load_web(url: str) -> List[Document]:\n",
    "    loader = WebBaseLoader(url)\n",
    "    documents = loader.load()  # loads the url\n",
    "    logger.info(f\"Loaded content from {url}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b51d3",
   "metadata": {},
   "source": [
    "## STEP 3 – Text Normalization\n",
    "\n",
    "To improve retrieval quality and embedding consistency, a normalization layer is applied before chunking.\n",
    "\n",
    "The cleaning function standardizes whitespace, removes excessive line breaks, and trims unnecessary spacing. This reduces noise and ensures more stable semantic representations during embedding.\n",
    "\n",
    "During preprocessing, each document is also assigned a unique `doc_id`. This enables:\n",
    "\n",
    "- Precise traceability across retrieval stages  \n",
    "- Debugging and evaluation tracking  \n",
    "- Source attribution in generated responses  \n",
    "\n",
    "This normalization step ensures the system maintains enterprise-level consistency and document identity control before entering the embedding pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c9728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Normalize Windows/Mac line endings\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Remove excessive whitespace (but preserve paragraphs)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    # Remove repeated standalone page numbers\n",
    "    text = re.sub(r\"\\n\\d+\\n\", \"\\n\", text)\n",
    "\n",
    "    # Remove lines that are only numeric/axis garbage\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Skip empty numeric axis lines like: \"0 20 40 60 80\"\n",
    "        if re.match(r\"^[\\d\\.\\%\\-\\s]+$\", stripped):\n",
    "            continue\n",
    "\n",
    "        # Skip very short garbage lines\n",
    "        if len(stripped) < 2:\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    # Normalize multiple blank lines to max 2\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f9e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "def preprocess_documents(documents: List[Document]) -> List[Document]:\n",
    "    for doc in documents:\n",
    "        # Clean content\n",
    "        cleaned = clean_text(doc.page_content)\n",
    "\n",
    "        # Skip empty content\n",
    "        if not cleaned or len(cleaned.strip()) < 20:\n",
    "            doc.page_content = \"\"\n",
    "            continue\n",
    "\n",
    "        doc.page_content = cleaned\n",
    "\n",
    "        # Ensure metadata exists\n",
    "        if not hasattr(doc, \"metadata\") or doc.metadata is None:\n",
    "            doc.metadata = {}\n",
    "\n",
    "        # Add doc_id only if not already present\n",
    "        if \"doc_id\" not in doc.metadata:\n",
    "            doc.metadata[\"doc_id\"] = str(uuid.uuid4())\n",
    "\n",
    "    # Remove empty documents after cleaning\n",
    "    documents = [doc for doc in documents if doc.page_content.strip()]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e533a4",
   "metadata": {},
   "source": [
    "## STEP 4 – Chunking Strategy 1: Recursive\n",
    "\n",
    "The baseline chunking strategy uses recursive character-based splitting to divide documents into manageable segments before embedding.\n",
    "\n",
    "This method creates overlapping chunks to preserve contextual continuity across boundaries. It is computationally efficient and widely used in production RAG systems as a strong default approach.\n",
    "\n",
    "**Trade-offs**\n",
    "\n",
    "✔ Fast and scalable  \n",
    "✔ Simple to configure and maintain  \n",
    "✖ Does not explicitly respect semantic or structural boundaries  \n",
    "\n",
    "Recursive chunking serves as a reliable baseline for retrieval performance before experimenting with more advanced semantic or structure-aware chunking strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e919ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 'Recursive_chunking' \n",
    "def recursive_chunking(documents: List[Document], \n",
    "                       chunk_size=800, \n",
    "                       chunk_overlap=150): \n",
    "                       splitter = RecursiveCharacterTextSplitter( chunk_size=chunk_size, \n",
    "                                                                 chunk_overlap=chunk_overlap ) \n",
    "                       \n",
    "                       chunks = splitter.split_documents(documents) \n",
    "                       logger.info(f\"Recursive chunking produced {len(chunks)} chunks.\")\n",
    "                       \n",
    "                       return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71ba16",
   "metadata": {},
   "source": [
    "## STEP 5 – Chunking Strategy 2: Sliding Window\n",
    "\n",
    "The sliding window strategy manually segments documents using a fixed window size with controlled overlap. Unlike recursive splitting, this approach explicitly defines how much context is preserved between consecutive chunks.\n",
    "\n",
    "Each chunk is generated by moving a fixed-size window across the document text while retaining a configurable overlap. This provides more predictable chunk boundaries and stronger contextual continuity.\n",
    "\n",
    "**Trade-offs**\n",
    "\n",
    "✔ Precise control over overlap  \n",
    "✔ Improved context retention across chunk boundaries  \n",
    "✖ Still character-based and not semantically aware  \n",
    "\n",
    "This strategy offers better contextual consistency compared to basic recursive splitting, while remaining computationally efficient for large-scale document processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47dbd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_chunking(documents: List[Document],\n",
    "                            window_size=800,\n",
    "                            overlap=200):\n",
    "    \n",
    "    chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "        start = 0\n",
    "\n",
    "        while start < len(text):\n",
    "            end = start + window_size\n",
    "            chunk_text = text[start:end]\n",
    "\n",
    "            chunks.append(\n",
    "                Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "            )\n",
    "\n",
    "            start += window_size - overlap\n",
    "    \n",
    "    logger.info(f\"Sliding window produced {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e333ed",
   "metadata": {},
   "source": [
    "## STEP 6 – Chunking Strategy 3: Semantic Chunking\n",
    "\n",
    "The semantic chunking strategy segments documents based on embedding similarity rather than fixed character limits. Sentences are first embedded, and consecutive sentences are grouped together only if their semantic similarity exceeds a defined threshold.\n",
    "\n",
    "This approach dynamically determines chunk boundaries by measuring contextual coherence between adjacent sentences. When similarity drops below the threshold, a new chunk is created.\n",
    "\n",
    "**Trade-offs**\n",
    "\n",
    "✔ Preserves semantic boundaries more effectively  \n",
    "✔ Improves retrieval precision and relevance  \n",
    "✖ Computationally slower than character-based methods  \n",
    "✖ Higher embedding cost during preprocessing  \n",
    "\n",
    "Semantic chunking is particularly useful in high-accuracy enterprise RAG systems where retrieval quality is prioritized over preprocessing speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8dedd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "W0221 18:03:15.247000 4084 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# define 'semantic_chunking' function\n",
    "def semantic_chunking(documents: List[Document],\n",
    "                      similarity_threshold=0.75):\n",
    "\n",
    "    chunks = []\n",
    "    \n",
    "    # iterate over documents\n",
    "    for doc in documents:\n",
    "        sentences = doc.page_content.split(\". \")\n",
    "        embeddings = embedding_model.encode(sentences)\n",
    "        \n",
    "        current_chunk = [sentences[0]]\n",
    "        \n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity(\n",
    "                [embeddings[i]],\n",
    "                [embeddings[i-1]]\n",
    "            )[0][0]\n",
    "            \n",
    "            if sim > similarity_threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\n",
    "                    Document(\n",
    "                        page_content=\". \".join(current_chunk),\n",
    "                        metadata=doc.metadata\n",
    "                    )\n",
    "                )\n",
    "                current_chunk = [sentences[i]]\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\n",
    "                Document(\n",
    "                    page_content=\". \".join(current_chunk),\n",
    "                    metadata=doc.metadata\n",
    "                )\n",
    "            )\n",
    "\n",
    "    logger.info(f\"Semantic chunking produced {len(chunks)} chunks.\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbda94c",
   "metadata": {},
   "source": [
    "## STEP 7 – Chunk Analytics\n",
    "\n",
    "To objectively compare chunking strategies, a simple analytics layer is introduced to evaluate chunk distribution characteristics.\n",
    "\n",
    "For each strategy, we measure:\n",
    "\n",
    "- Total number of chunks  \n",
    "- Average chunk length  \n",
    "- Minimum chunk length  \n",
    "- Maximum chunk length  \n",
    "\n",
    "These metrics help assess how aggressively a strategy segments documents and how consistent the chunk sizes are.  \n",
    "\n",
    "Chunk analytics is critical because retrieval performance is directly influenced by chunk granularity. Extremely small chunks may lose context, while overly large chunks may dilute semantic precision.\n",
    "\n",
    "This evaluation step provides quantitative insight before selecting the optimal chunking strategy for the Hybrid RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4d5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks: List[Document], label: str):\n",
    "    lengths = [len(c.page_content) for c in chunks]\n",
    "    \n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"Total chunks:\", len(chunks))\n",
    "    print(\"Avg length:\", np.mean(lengths))\n",
    "    print(\"Min length:\", np.min(lengths))\n",
    "    print(\"Max length:\", np.max(lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d593e5d",
   "metadata": {},
   "source": [
    "### Chunking Strategy Comparison\n",
    "\n",
    "The analytics function is executed for Recursive, Sliding Window, and Semantic chunking strategies to compare their distribution patterns.\n",
    "\n",
    "This helps evaluate chunk count and size consistency, providing insight into context preservation and potential retrieval impact before selecting the optimal strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6823e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 457 pages from PDF.\n",
      "INFO:__main__:Recursive chunking produced 1333 chunks.\n",
      "INFO:__main__:Sliding window produced 1537 chunks.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 66.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 50.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 48.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 63.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 95.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 77.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.76it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.84it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.26it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.96it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.46it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.54it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.44it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  6.89it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  8.49it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.54it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  4.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.42it/s]\n",
      "INFO:__main__:Semantic chunking produced 4212 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recursive\n",
      "Total chunks: 1333\n",
      "Avg length: 660.9024756189048\n",
      "Min length: 32\n",
      "Max length: 800\n",
      "\n",
      "Sliding\n",
      "Total chunks: 1537\n",
      "Avg length: 637.7143786597268\n",
      "Min length: 1\n",
      "Max length: 800\n",
      "\n",
      "Semantic\n",
      "Total chunks: 4212\n",
      "Avg length: 183.41856600189934\n",
      "Min length: 1\n",
      "Max length: 4524\n"
     ]
    }
   ],
   "source": [
    "# Example test file\n",
    "documents = load_pdf(r\"C:\\my_projects\\enterprise-rag\\data\\Stanford AI Index Report.pdf\")\n",
    "documents = preprocess_documents(documents)\n",
    "\n",
    "# Generate chunks\n",
    "recursive_chunks = recursive_chunking(documents)\n",
    "sliding_chunks = sliding_window_chunking(documents)\n",
    "semantic_chunks = semantic_chunking(documents)\n",
    "\n",
    "# Analyze\n",
    "analyze_chunks(recursive_chunks, \"Recursive\")\n",
    "analyze_chunks(sliding_chunks, \"Sliding\")\n",
    "analyze_chunks(semantic_chunks, \"Semantic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613a0de",
   "metadata": {},
   "source": [
    "## Chunking Strategy Decision\n",
    "\n",
    "After evaluating all three approaches:\n",
    "\n",
    "- Recursive chunking provides stable and consistent chunk sizes.  \n",
    "- Sliding window produces uneven tail chunks due to fixed window shifts.  \n",
    "- Semantic chunking significantly over-fragments the document (205 chunks), increasing preprocessing cost.\n",
    "\n",
    "For production balance between latency, stability, and contextual coherence, **recursive chunking** is selected as the default strategy for embedding experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4c814",
   "metadata": {},
   "source": [
    "## Persisting Processed Chunks\n",
    "\n",
    "The final chunk dataset is serialized and saved using `pickle` for reuse in downstream stages.\n",
    "\n",
    "Persisting the processed chunks avoids repeated ingestion and chunking overhead, enabling faster experimentation with embeddings, retrieval strategies, and reranking without recomputing preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8db12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"processed_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(recursive_chunks, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72a1b6",
   "metadata": {},
   "source": [
    "## Data Ingestion & Chunking Pipeline summary\n",
    "\n",
    "This notebook established the foundational preprocessing layer of the Hybrid RAG system.\n",
    "\n",
    "Completed stages include:\n",
    "\n",
    "- Modular document ingestion (PDF, CSV, Web)  \n",
    "- Text normalization with document identity tracking  \n",
    "- Implementation of three chunking strategies (Recursive, Sliding Window, Semantic)  \n",
    "- Quantitative chunk analytics for strategy comparison  \n",
    "- Persistence of processed chunk dataset for downstream reuse  \n",
    "\n",
    "The system is now ready for the next phase: embedding generation, indexing, and hybrid retrieval architecture design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf90e468",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
